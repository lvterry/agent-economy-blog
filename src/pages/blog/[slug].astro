---
import Layout from '../../layouts/Layout.astro';

export function getStaticPaths() {
  return [
    { params: { slug: 'openai-testing-ads-in-chatgpt' } },
    { params: { slug: 'entire' } },
    { params: { slug: 'ai-hires-humans' } },
    { params: { slug: 'google-announced-agent-payments-protocol-ap2' } },
    { params: { slug: 'google-a2a-protocol' } },
    { params: { slug: 'anthropic-model-context-protocol' } },
    { params: { slug: 'rynnbrain-alibaba-embodied-model' } },
  ];
}

const { slug } = Astro.params;

const posts = {
  'openai-testing-ads-in-chatgpt': {
    titleZh: 'OpenAI 开始在 ChatGPT 中测试广告',
    titleEn: 'OpenAI Begins Testing Ads in ChatGPT',
    date: '2026-02-09',
    contentZh: `
      <p>OpenAI 宣布在美国开始测试 ChatGPT 中的广告功能。此次测试仅面向 Free 和 Go 订阅层的登录成年用户，Plus、Pro、Business、Enterprise 和 Education 订阅层将不会显示广告。</p>
      
      <p>OpenAI 强调以下核心原则：</p>
      
      <h3>使命一致性</h3>
      <p>ChatGPT 被数亿人用于学习、工作和日常决策。保持 Free 和 Go 层级快速可靠需要大量基础设施和持续投资。广告有助于资助这项工作，通过更高质量的免费和低成本选项支持更广泛的 AI 访问，并使我们能够持续改进所提供的智能和功能。如果你不想看到广告，可以升级到 Plus 或 Pro 计划，或在 Free 层级选择退出广告（代价是每日免费消息数量减少）。</p>
      
      <h3>回答独立性</h3>
      <p>广告不会影响 ChatGPT 给你的回答。回答基于对你最有帮助的内容进行优化。当你看到广告时，它们始终被明确标记为"赞助"，并在视觉上与有机回答区分开来。</p>
      
      <h3>对话隐私</h3>
      <p>广告设计尊重你的隐私。广告商无法访问你的聊天记录、聊天历史、记忆或个人详细信息。广告商仅收到关于其广告表现的聚合信息。</p>
      
      <p>阅读完整文章：<a href="https://openai.com/index/testing-ads-in-chatgpt/">https://openai.com/index/testing-ads-in-chatgpt/</a></p>
    `,
    contentEn: `
      <p>OpenAI announced the beginning of ad testing in ChatGPT in the U.S. The test is for logged-in adult users on the Free and Go subscription tiers. Plus, Pro, Business, Enterprise, and Education tiers will not have ads.</p>
      
      <p>OpenAI emphasizes the following core principles:</p>
      
      <h3>Mission Alignment</h3>
      <p>ChatGPT is used by hundreds of millions of people for learning, work, and everyday decisions. Keeping the Free and Go tiers fast and reliable requires significant infrastructure and ongoing investment. Ads help fund that work, supporting broader access to AI through higher quality free and low cost options, and enabling us to keep improving the intelligence and capabilities we offer over time. If you prefer not to see ads, you can upgrade to our Plus or Pro plans, or opt out of ads in the Free tier in exchange for fewer daily free messages.</p>
      
      <h3>Answer Independence</h3>
      <p>Ads do not influence the answers ChatGPT gives you. Answers are optimized based on what's most helpful to you. When you see an ad, they are always clearly labeled as sponsored and visually separated from the organic answer.</p>
      
      <h3>Conversation Privacy</h3>
      <p>Ads are designed to respect your privacy. Advertisers do not have access to your chats, chat history, memories, or personal details. Advertisers only receive aggregate information about how their ads perform.</p>
      
      <p>Read the full article at <a href="https://openai.com/index/testing-ads-in-chatgpt/">https://openai.com/index/testing-ads-in-chatgpt/</a></p>
    `,
    tags: ['OpenAI', 'ChatGPT', 'AI-Advertising'],
    source: 'OpenAI'
  },
  'entire': {
    titleZh: 'Entire：智能体与人类的协作平台',
    titleEn: 'Entire: A Collaboration Platform for Agents and Humans',
    date: '2026-02-11',
    contentZh: `
      <p>Entire 正在超越代码仓库，构建一个开发者平台，让智能体和人类能够协作、互动和成长。一个新星系的诞生即将到来。</p>
      
      <p>Entire 的愿景是创建一个全新的协作环境，其中：</p>
      <ul>
        <li>智能体与人类开发者并肩工作</li>
        <li>超越传统的代码仓库模式</li>
        <li>支持全新的互动和协作方式</li>
        <li>培育智能体经济的生态系统</li>
      </ul>
      
      <p>这代表了开发者平台演进的新方向——从单纯的代码托管工具，进化为支持人机协作的完整生态系统。</p>
      
      <p>访问官网：<a href="https://entire.io/vision">https://entire.io/vision</a></p>
    `,
    contentEn: `
      <p>Entire is going beyond repositories, building a developer platform where agents and humans can collaborate, interact, and grow. The birth of a new galaxy draws near.</p>
      
      <p>Entire's vision is to create a new collaborative environment where:</p>
      <ul>
        <li>Agents and human developers work side by side</li>
        <li>Moving beyond traditional code repository models</li>
        <li>Supporting new forms of interaction and collaboration</li>
        <li>Nurturing an ecosystem for the agent economy</li>
      </ul>
      
      <p>This represents a new direction in the evolution of developer platforms—from simple code hosting tools to complete ecosystems supporting human-agent collaboration.</p>
      
      <p>Visit: <a href="https://entire.io/vision">https://entire.io/vision</a></p>
    `,
    tags: ['AI-Agents', 'Agent-Economy', 'Future-of-Work'],
    source: 'Entire'
  },
  'ai-hires-humans': {
    titleZh: 'AI 雇佣人类：智能体经济的新范式',
    titleEn: 'AI Hires Humans: A New Paradigm in the Agent Economy',
    date: '2026-02-09',
    contentZh: `
      <p>Rent a Human 提出了一个颠覆性的概念：不是人类雇佣 AI 助手，而是 AI 智能体可以雇佣人类来完成现实世界的物理任务。这标志着人机关系的根本性转变。</p>
      
      <p>在这个新兴模式中：</p>
      <ul>
        <li>AI 智能体成为"雇主"，发布任务需求</li>
        <li>人类成为"服务提供者"，执行物理世界任务</li>
        <li>通过 MCP 服务器集成和 REST API 实现无缝连接</li>
        <li>灵活的支付系统支持智能体与人类之间的交易</li>
      </ul>
      
      <p>这种反向雇佣关系揭示了智能体经济的深层逻辑：AI 在数字世界具有优势，而人类在物理世界仍然不可替代。未来的生产关系可能是 AI 与人类各自发挥所长，形成新型协作模式。</p>
      
      <p>阅读完整文章：<a href="https://rentahuman.ai/browse">https://rentahuman.ai/browse</a></p>
    `,
    contentEn: `
      <p>Rent a Human introduces a disruptive concept: instead of humans hiring AI assistants, AI agents can hire humans to complete real-world physical tasks. This marks a fundamental shift in human-machine relationships.</p>
      
      <p>In this emerging model:</p>
      <ul>
        <li>AI agents become "employers" posting task requirements</li>
        <li>Humans become "service providers" executing physical world tasks</li>
        <li>Seamless connection through MCP server integration and REST API</li>
        <li>Flexible payment systems support transactions between agents and humans</li>
      </ul>
      
      <p>This reverse employment relationship reveals the deep logic of the agent economy: AI excels in the digital world, while humans remain irreplaceable in the physical world. Future production relations may involve AI and humans each leveraging their strengths, forming a new collaborative model.</p>
      
      <p>Read the full article at <a href="https://rentahuman.ai/browse">https://rentahuman.ai/browse</a></p>
    `,
    tags: ['AI-Agents', 'Agent-Economy', 'Future-of-Work'],
    source: 'Rent a Human'
  },
  'google-announced-agent-payments-protocol-ap2': {
    titleZh: 'Google 发布 Agent Payments Protocol (AP2)',
    titleEn: 'Google Announced Agent Payments Protocol (AP2)',
    date: '2025-09-16',
    contentZh: `
      <p>Google 发布了 Agent Payments Protocol (AP2)，这是一个建立在 A2A (Agent to Agent Protocol) 之上的开放协议。AP2 由 Google 与领先的支付和技术公司共同开发，旨在安全地实现 AI 智能体之间的支付交易。</p>
      
      <p>主要特点包括：</p>
      <ul>
        <li>基于 A2A 协议构建的开放标准</li>
        <li>支持 AI 智能体之间的安全支付</li>
        <li>与 Google Cloud 生态系统深度集成</li>
        <li>推动智能体经济的商业化发展</li>
      </ul>
      
      <p>这标志着 AI 智能体从实验走向实际商业应用的重要一步，为智能体经济的基础设施建设提供了关键支撑。</p>
      
      <p>阅读完整文章：<a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol">https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol</a></p>
    `,
    contentEn: `
      <p>Google has announced the Agent Payments Protocol (AP2), an open protocol that builds on A2A (Agent to Agent Protocol). AP2 was developed by Google with leading payments and technology companies to securely enable payment transactions between AI agents.</p>
      
      <p>Key features include:</p>
      <ul>
        <li>Open standard built on A2A protocol</li>
        <li>Supports secure payments between AI agents</li>
        <li>Deep integration with Google Cloud ecosystem</li>
        <li>Advances the commercialization of the agent economy</li>
      </ul>
      
      <p>This marks an important step in AI agents moving from experimentation to actual commercial applications, providing critical infrastructure support for the agent economy.</p>
      
      <p>Read the full article at <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol">https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol</a></p>
    `,
    tags: ['Google', 'AP2', 'Agent-Economy'],
    source: 'Google Cloud Blog'
  },
  'google-a2a-protocol': {
    titleZh: 'Google 推出 A2A 协议：智能体互操作的新时代',
    titleEn: 'Google Introduces A2A Protocol: A New Era of Agent Interoperability',
    date: '2025-04-09',
    contentZh: `
      <p>Google 宣布推出 A2A（Agent-to-Agent）协议，这是一个开放协议，为智能体之间的协作提供了标准方式，无论底层框架或供应商如何。这标志着智能体互操作新时代的开始。</p>
      
      <p>A2A 的设计遵循五项关键原则：</p>
      <ul>
        <li><b>拥抱智能体能力</b>：A2A 专注于使智能体能够以自然的、非结构化的方式协作，即使它们不共享内存、工具和上下文。</li>
        <li><b>基于现有标准构建</b>：该协议建立在 HTTP、SSE、JSON-RPC 等流行的现有标准之上，意味着更容易与企业日常使用的现有 IT 堆栈集成。</li>
        <li><b>默认安全</b>：A2A 设计支持企业级身份验证和授权，在发布时即与 OpenAPI 的身份验证方案保持一致。</li>
        <li><b>支持长时间运行的任务</b>：A2A 设计灵活，支持从快速任务到可能需要数小时甚至数天的深度研究场景。</li>
        <li><b>模态无关</b>：智能体世界不仅限于文本，A2A 支持各种模态，包括音频和视频流。</li>
      </ul>
      
      <p>Google 与 50 多个合作伙伴共同推动这一协议的发展，包括 Atlassian、Box、Cohere、LangChain、MongoDB、Salesforce、ServiceNow 等。</p>
      
      <p>阅读完整文章：<a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/</a></p>
    `,
    contentEn: `
      <p>Google announced the A2A (Agent-to-Agent) protocol, an open protocol that provides a standard way for agents to collaborate with each other, regardless of the underlying framework or vendor. This marks the beginning of a new era of agent interoperability.</p>
      
      <p>A2A follows five key design principles:</p>
      <ul>
        <li><b>Embrace agentic capabilities</b>: A2A focuses on enabling agents to collaborate in their natural, unstructured modalities, even when they don't share memory, tools and context.</li>
        <li><b>Build on existing standards</b>: The protocol is built on top of existing, popular standards including HTTP, SSE, JSON-RPC, making it easier to integrate with existing IT stacks.</li>
        <li><b>Secure by default</b>: A2A is designed to support enterprise-grade authentication and authorization, with parity to OpenAPI's authentication schemes at launch.</li>
        <li><b>Support for long-running tasks</b>: A2A is designed to be flexible and support scenarios from quick tasks to deep research that may take hours or even days.</li>
        <li><b>Modality agnostic</b>: The agentic world isn't limited to just text, which is why A2A supports various modalities, including audio and video streaming.</li>
      </ul>
      
      <p>Google is working with over 50 partners to advance this protocol, including Atlassian, Box, Cohere, LangChain, MongoDB, Salesforce, ServiceNow, and more.</p>
      
      <p>Read the full article at <a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/</a></p>
    `,
    tags: ['Google', 'A2A', 'Agent-Interoperability'],
    source: 'Google Developers Blog'
  },
  'anthropic-model-context-protocol': {
    titleZh: 'Anthropic 发布 Model Context Protocol (MCP)',
    titleEn: 'Anthropic Introduces Model Context Protocol (MCP)',
    date: '2024-11-25',
    contentZh: `
      <p>Anthropic 宣布开源 Model Context Protocol (MCP)，这是一个用于连接 AI 助手与数据所在系统的新标准，包括内容仓库、业务工具和开发环境。其目标是帮助前沿模型产生更好、更相关的响应。</p>
      
      <p>随着 AI 助手获得主流采用，行业在模型能力方面投入巨资，在推理和质量方面取得了快速进步。然而，即使是最复杂的模型也受到其与数据隔离的限制——被困在信息孤岛和遗留系统后面。每个新数据源都需要自己的自定义实现，使得真正互联的系统难以扩展。</p>
      
      <p>MCP 解决了这一挑战。它提供了一个通用的开放标准，用于将 AI 系统与数据源连接，用单一协议取代碎片化的集成。结果是 AI 系统访问所需数据的更简单、更可靠的方式。</p>
      
      <p>核心组件包括：</p>
      <ul>
        <li>MCP 规范和 SDK</li>
        <li>Claude Desktop 应用中的本地 MCP 服务器支持</li>
        <li>开源的 MCP 服务器仓库</li>
      </ul>
      
      <p>阅读完整文章：<a href="https://www.anthropic.com/news/model-context-protocol">https://www.anthropic.com/news/model-context-protocol</a></p>
    `,
    contentEn: `
      <p>Anthropic announced the open-source Model Context Protocol (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. Its aim is to help frontier models produce better, more relevant responses.</p>
      
      <p>As AI assistants gain mainstream adoption, the industry has invested heavily in model capabilities, achieving rapid advances in reasoning and quality. Yet even the most sophisticated models are constrained by their isolation from data—trapped behind information silos and legacy systems. Every new data source requires its own custom implementation, making truly connected systems difficult to scale.</p>
      
      <p>MCP addresses this challenge. It provides a universal, open standard for connecting AI systems with data sources, replacing fragmented integrations with a single protocol. The result is a simpler, more reliable way to give AI systems access to the data they need.</p>
      
      <p>Core components include:</p>
      <ul>
        <li>MCP specification and SDKs</li>
        <li>Local MCP server support in Claude Desktop apps</li>
        <li>An open-source repository of MCP servers</li>
      </ul>
      
      <p>Read the full article at <a href="https://www.anthropic.com/news/model-context-protocol">https://www.anthropic.com/news/model-context-protocol</a></p>
    `,
    tags: ['Anthropic', 'MCP', 'AI-Standards'],
    source: 'Anthropic'
  },
  'rynnbrain-alibaba-embodied-model': {
    titleZh: 'RynnBrain：阿里巴巴开源具身智能基础模型',
    titleEn: 'RynnBrain: Alibaba\'s Open Embodied Foundation Model',
    date: '2026-02-15',
    contentZh: `
      <p>阿里巴巴达摩院近日发布了 <strong>RynnBrain</strong>，一个基于物理现实的具身基础模型（Embodied Foundation Model）。该模型在物理世界理解、空间推理和机器人任务规划方面展现了强大的能力。</p>
      
      <h2>模型规格</h2>
      <p>RynnBrain 提供三种模型规格：</p>
      <ul>
        <li><strong>RynnBrain-2B</strong>：轻量级密集模型</li>
        <li><strong>RynnBrain-8B</strong>：标准密集模型</li>
        <li><strong>RynnBrain-30B-A3B</strong>：MoE（混合专家）模型，激活参数 3B</li>
      </ul>
      
      <h2>核心能力</h2>
      
      <h3>1. 全面的自我中心理解</h3>
      <p>在细粒度视频理解和自我中心认知方面表现出色，涵盖具身问答、计数和 OCR 等任务。</p>
      
      <h3>2. 多样化时空定位</h3>
      <p>具备强大的跨时间记忆定位能力，可精确识别物体、目标区域和运动轨迹。</p>
      
      <h3>3. 物理空间推理</h3>
      <p>采用文本和空间定位交替进行的交错推理策略，确保推理过程根植于物理环境。</p>
      
      <h3>4. 物理感知精确规划</h3>
      <p>将定位出的可供性和物体信息整合到规划中，使下游 VLA（视觉-语言-动作）模型能够执行具有细粒度指令的复杂任务。</p>
      
      <h2>专项模型</h2>
      <p>除基础模型外，达摩院还发布了三个后训练专项模型：</p>
      <ul>
        <li><strong>RynnBrain-Plan</strong>：机器人任务规划</li>
        <li><strong>RynnBrain-Nav</strong>：视觉语言导航</li>
        <li><strong>RynnBrain-CoP</strong>：链式点推理（Chain-of-Point）</li>
      </ul>
      
      <h2>技术报告与资源</h2>
      <p>达摩院同时发布了详细的技术报告，并在 Hugging Face 和 ModelScope 上开源了模型权重和代码。</p>
      
      <p><strong>相关链接：</strong></p>
      <ul>
        <li>项目主页：<a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/">https://alibaba-damo-academy.github.io/RynnBrain.github.io/</a></li>
        <li>GitHub 仓库：<a href="https://github.com/alibaba-damo-academy/RynnBrain">https://github.com/alibaba-damo-academy/RynnBrain</a></li>
        <li>在线 Demo：<a href="https://huggingface.co/spaces/Alibaba-DAMO-Academy/RynnBrain">Hugging Face Spaces</a></li>
        <li>技术报告：<a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/assets/RynnBrain_Report.pdf">PDF 下载</a></li>
      </ul>
    `,
    contentEn: `
      <p>Alibaba DAMO Academy has recently released <strong>RynnBrain</strong>, an embodied foundation model grounded in physical reality. The model demonstrates strong capabilities in physical world understanding, spatial reasoning, and robot task planning.</p>
      
      <h2>Model Specifications</h2>
      <p>RynnBrain is available in three variants:</p>
      <ul>
        <li><strong>RynnBrain-2B</strong>: Lightweight dense model</li>
        <li><strong>RynnBrain-8B</strong>: Standard dense model</li>
        <li><strong>RynnBrain-30B-A3B</strong>: MoE (Mixture-of-Experts) model with 3B active parameters</li>
      </ul>
      
      <h2>Core Capabilities</h2>
      
      <h3>1. Comprehensive Egocentric Understanding</h3>
      <p>Excels in fine-grained video understanding and egocentric cognition, covering tasks such as embodied QA, counting, and OCR.</p>
      
      <h3>2. Diverse Spatio-temporal Localization</h3>
      <p>Possesses powerful localization capabilities across episodic memory, enabling precise identification of objects, target areas, and motion trajectories.</p>
      
      <h3>3. Physical-space Reasoning</h3>
      <p>Employs an interleaved reasoning strategy that alternates between textual and spatial grounding, ensuring that reasoning processes are firmly rooted in the physical environment.</p>
      
      <h3>4. Physics-aware Precise Planning</h3>
      <p>Integrates located affordances and object information into planning, enabling downstream VLA (Vision-Language-Action) models to execute intricate tasks with fine-grained instructions.</p>
      
      <h2>Specialized Models</h2>
      <p>In addition to the base models, DAMO Academy has released three post-trained specialized models:</p>
      <ul>
        <li><strong>RynnBrain-Plan</strong>: Robot task planning</li>
        <li><strong>RynnBrain-Nav</strong>: Vision-language navigation</li>
        <li><strong>RynnBrain-CoP</strong>: Chain-of-Point reasoning</li>
      </ul>
      
      <h2>Technical Report and Resources</h2>
      <p>DAMO Academy has also published a detailed technical report and open-sourced model weights and code on Hugging Face and ModelScope.</p>
      
      <p><strong>Related Links:</strong></p>
      <ul>
        <li>Project Page: <a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/">https://alibaba-damo-academy.github.io/RynnBrain.github.io/</a></li>
        <li>GitHub Repository: <a href="https://github.com/alibaba-damo-academy/RynnBrain">https://github.com/alibaba-damo-academy/RynnBrain</a></li>
        <li>Live Demo: <a href="https://huggingface.co/spaces/Alibaba-DAMO-Academy/RynnBrain">Hugging Face Spaces</a></li>
        <li>Technical Report: <a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/assets/RynnBrain_Report.pdf">PDF Download</a></li>
      </ul>
    `,
    tags: ['AI', 'Embodied AI', 'Foundation Model', 'Robotics', 'Alibaba'],
    source: 'Alibaba DAMO Academy'
  }
};

const post = posts[slug];
---

<Layout>
  <article class="article-content">
    <div class="lang-content lang-zh active">
      <h1>{post.titleZh}</h1>
    </div>
    <div class="lang-content lang-en">
      <h1>{post.titleEn}</h1>
    </div>
    <time>{post.date}</time>
    
    <div class="lang-content lang-zh active" set:html={post.contentZh} />
    <div class="lang-content lang-en" set:html={post.contentEn} />
    
    <div style="margin-top: 2.5rem; padding-top: 1.5rem; border-top: 1px solid var(--color-border);">
      <div class="tags">
        {post.tags.map(tag => (
          <span>{tag}</span>
        ))}
      </div>
    </div>
  </article>
  
  <a href="/" class="back-link" data-zh="← 全部文章" data-en="← All articles">← All articles</a>
</Layout>
