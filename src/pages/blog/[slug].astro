---
import Layout from '../../layouts/Layout.astro';

export function getStaticPaths() {
  return [
    { params: { slug: 'openai-testing-ads-in-chatgpt' } },
    { params: { slug: 'entire' } },
    { params: { slug: 'ai-hires-humans' } },
    { params: { slug: 'google-announced-agent-payments-protocol-ap2' } },
    { params: { slug: 'google-a2a-protocol' } },
    { params: { slug: 'anthropic-model-context-protocol' } },
    { params: { slug: 'rynnbrain-alibaba-embodied-model' } },
    { params: { slug: 'figma-future-of-design-code-canvas' } },
    { params: { slug: 'gemini-3-1-pro-model-card' } },
  ];
}

const { slug } = Astro.params;

const posts = {
  'openai-testing-ads-in-chatgpt': {
    titleZh: 'OpenAI 开始在 ChatGPT 中测试广告',
    titleEn: 'OpenAI Begins Testing Ads in ChatGPT',
    date: '2026-02-09',
    contentZh: `
      <p>OpenAI 宣布在美国开始测试 ChatGPT 中的广告功能。此次测试仅面向 Free 和 Go 订阅层的登录成年用户，Plus、Pro、Business、Enterprise 和 Education 订阅层将不会显示广告。</p>
      
      <p>OpenAI 强调以下核心原则：</p>
      
      <h3>使命一致性</h3>
      <p>ChatGPT 被数亿人用于学习、工作和日常决策。保持 Free 和 Go 层级快速可靠需要大量基础设施和持续投资。广告有助于资助这项工作，通过更高质量的免费和低成本选项支持更广泛的 AI 访问，并使我们能够持续改进所提供的智能和功能。如果你不想看到广告，可以升级到 Plus 或 Pro 计划，或在 Free 层级选择退出广告（代价是每日免费消息数量减少）。</p>
      
      <h3>回答独立性</h3>
      <p>广告不会影响 ChatGPT 给你的回答。回答基于对你最有帮助的内容进行优化。当你看到广告时，它们始终被明确标记为"赞助"，并在视觉上与有机回答区分开来。</p>
      
      <h3>对话隐私</h3>
      <p>广告设计尊重你的隐私。广告商无法访问你的聊天记录、聊天历史、记忆或个人详细信息。广告商仅收到关于其广告表现的聚合信息。</p>
      
      <p>阅读完整文章：<a href="https://openai.com/index/testing-ads-in-chatgpt/">https://openai.com/index/testing-ads-in-chatgpt/</a></p>
    `,
    contentEn: `
      <p>OpenAI announced the beginning of ad testing in ChatGPT in the U.S. The test is for logged-in adult users on the Free and Go subscription tiers. Plus, Pro, Business, Enterprise, and Education tiers will not have ads.</p>
      
      <p>OpenAI emphasizes the following core principles:</p>
      
      <h3>Mission Alignment</h3>
      <p>ChatGPT is used by hundreds of millions of people for learning, work, and everyday decisions. Keeping the Free and Go tiers fast and reliable requires significant infrastructure and ongoing investment. Ads help fund that work, supporting broader access to AI through higher quality free and low cost options, and enabling us to keep improving the intelligence and capabilities we offer over time. If you prefer not to see ads, you can upgrade to our Plus or Pro plans, or opt out of ads in the Free tier in exchange for fewer daily free messages.</p>
      
      <h3>Answer Independence</h3>
      <p>Ads do not influence the answers ChatGPT gives you. Answers are optimized based on what's most helpful to you. When you see an ad, they are always clearly labeled as sponsored and visually separated from the organic answer.</p>
      
      <h3>Conversation Privacy</h3>
      <p>Ads are designed to respect your privacy. Advertisers do not have access to your chats, chat history, memories, or personal details. Advertisers only receive aggregate information about how their ads perform.</p>
      
      <p>Read the full article at <a href="https://openai.com/index/testing-ads-in-chatgpt/">https://openai.com/index/testing-ads-in-chatgpt/</a></p>
    `,
    tags: ['OpenAI', 'ChatGPT', 'AI-Advertising'],
    source: 'OpenAI'
  },
  'entire': {
    titleZh: 'Entire：智能体与人类的协作平台',
    titleEn: 'Entire: A Collaboration Platform for Agents and Humans',
    date: '2026-02-11',
    contentZh: `
      <p>Entire 正在超越代码仓库，构建一个开发者平台，让智能体和人类能够协作、互动和成长。一个新星系的诞生即将到来。</p>
      
      <p>Entire 的愿景是创建一个全新的协作环境，其中：</p>
      <ul>
        <li>智能体与人类开发者并肩工作</li>
        <li>超越传统的代码仓库模式</li>
        <li>支持全新的互动和协作方式</li>
        <li>培育智能体经济的生态系统</li>
      </ul>
      
      <p>这代表了开发者平台演进的新方向——从单纯的代码托管工具，进化为支持人机协作的完整生态系统。</p>
      
      <p>访问官网：<a href="https://entire.io/vision">https://entire.io/vision</a></p>
    `,
    contentEn: `
      <p>Entire is going beyond repositories, building a developer platform where agents and humans can collaborate, interact, and grow. The birth of a new galaxy draws near.</p>
      
      <p>Entire's vision is to create a new collaborative environment where:</p>
      <ul>
        <li>Agents and human developers work side by side</li>
        <li>Moving beyond traditional code repository models</li>
        <li>Supporting new forms of interaction and collaboration</li>
        <li>Nurturing an ecosystem for the agent economy</li>
      </ul>
      
      <p>This represents a new direction in the evolution of developer platforms—from simple code hosting tools to complete ecosystems supporting human-agent collaboration.</p>
      
      <p>Visit: <a href="https://entire.io/vision">https://entire.io/vision</a></p>
    `,
    tags: ['AI-Agents', 'Agent-Economy', 'Future-of-Work'],
    source: 'Entire'
  },
  'ai-hires-humans': {
    titleZh: 'AI 雇佣人类：智能体经济的新范式',
    titleEn: 'AI Hires Humans: A New Paradigm in the Agent Economy',
    date: '2026-02-09',
    contentZh: `
      <p>Rent a Human 提出了一个颠覆性的概念：不是人类雇佣 AI 助手，而是 AI 智能体可以雇佣人类来完成现实世界的物理任务。这标志着人机关系的根本性转变。</p>
      
      <p>在这个新兴模式中：</p>
      <ul>
        <li>AI 智能体成为"雇主"，发布任务需求</li>
        <li>人类成为"服务提供者"，执行物理世界任务</li>
        <li>通过 MCP 服务器集成和 REST API 实现无缝连接</li>
        <li>灵活的支付系统支持智能体与人类之间的交易</li>
      </ul>
      
      <p>这种反向雇佣关系揭示了智能体经济的深层逻辑：AI 在数字世界具有优势，而人类在物理世界仍然不可替代。未来的生产关系可能是 AI 与人类各自发挥所长，形成新型协作模式。</p>
      
      <p>阅读完整文章：<a href="https://rentahuman.ai/browse">https://rentahuman.ai/browse</a></p>
    `,
    contentEn: `
      <p>Rent a Human introduces a disruptive concept: instead of humans hiring AI assistants, AI agents can hire humans to complete real-world physical tasks. This marks a fundamental shift in human-machine relationships.</p>
      
      <p>In this emerging model:</p>
      <ul>
        <li>AI agents become "employers" posting task requirements</li>
        <li>Humans become "service providers" executing physical world tasks</li>
        <li>Seamless connection through MCP server integration and REST API</li>
        <li>Flexible payment systems support transactions between agents and humans</li>
      </ul>
      
      <p>This reverse employment relationship reveals the deep logic of the agent economy: AI excels in the digital world, while humans remain irreplaceable in the physical world. Future production relations may involve AI and humans each leveraging their strengths, forming a new collaborative model.</p>
      
      <p>Read the full article at <a href="https://rentahuman.ai/browse">https://rentahuman.ai/browse</a></p>
    `,
    tags: ['AI-Agents', 'Agent-Economy', 'Future-of-Work'],
    source: 'Rent a Human'
  },
  'google-announced-agent-payments-protocol-ap2': {
    titleZh: 'Google 发布 Agent Payments Protocol (AP2)',
    titleEn: 'Google Announced Agent Payments Protocol (AP2)',
    date: '2025-09-16',
    contentZh: `
      <p>Google 发布了 Agent Payments Protocol (AP2)，这是一个建立在 A2A (Agent to Agent Protocol) 之上的开放协议。AP2 由 Google 与领先的支付和技术公司共同开发，旨在安全地实现 AI 智能体之间的支付交易。</p>
      
      <p>主要特点包括：</p>
      <ul>
        <li>基于 A2A 协议构建的开放标准</li>
        <li>支持 AI 智能体之间的安全支付</li>
        <li>与 Google Cloud 生态系统深度集成</li>
        <li>推动智能体经济的商业化发展</li>
      </ul>
      
      <p>这标志着 AI 智能体从实验走向实际商业应用的重要一步，为智能体经济的基础设施建设提供了关键支撑。</p>
      
      <p>阅读完整文章：<a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol">https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol</a></p>
    `,
    contentEn: `
      <p>Google has announced the Agent Payments Protocol (AP2), an open protocol that builds on A2A (Agent to Agent Protocol). AP2 was developed by Google with leading payments and technology companies to securely enable payment transactions between AI agents.</p>
      
      <p>Key features include:</p>
      <ul>
        <li>Open standard built on A2A protocol</li>
        <li>Supports secure payments between AI agents</li>
        <li>Deep integration with Google Cloud ecosystem</li>
        <li>Advances the commercialization of the agent economy</li>
      </ul>
      
      <p>This marks an important step in AI agents moving from experimentation to actual commercial applications, providing critical infrastructure support for the agent economy.</p>
      
      <p>Read the full article at <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol">https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol</a></p>
    `,
    tags: ['Google', 'AP2', 'Agent-Economy'],
    source: 'Google Cloud Blog'
  },
  'google-a2a-protocol': {
    titleZh: 'Google 推出 A2A 协议：智能体互操作的新时代',
    titleEn: 'Google Introduces A2A Protocol: A New Era of Agent Interoperability',
    date: '2025-04-09',
    contentZh: `
      <p>Google 宣布推出 A2A（Agent-to-Agent）协议，这是一个开放协议，为智能体之间的协作提供了标准方式，无论底层框架或供应商如何。这标志着智能体互操作新时代的开始。</p>
      
      <p>A2A 的设计遵循五项关键原则：</p>
      <ul>
        <li><b>拥抱智能体能力</b>：A2A 专注于使智能体能够以自然的、非结构化的方式协作，即使它们不共享内存、工具和上下文。</li>
        <li><b>基于现有标准构建</b>：该协议建立在 HTTP、SSE、JSON-RPC 等流行的现有标准之上，意味着更容易与企业日常使用的现有 IT 堆栈集成。</li>
        <li><b>默认安全</b>：A2A 设计支持企业级身份验证和授权，在发布时即与 OpenAPI 的身份验证方案保持一致。</li>
        <li><b>支持长时间运行的任务</b>：A2A 设计灵活，支持从快速任务到可能需要数小时甚至数天的深度研究场景。</li>
        <li><b>模态无关</b>：智能体世界不仅限于文本，A2A 支持各种模态，包括音频和视频流。</li>
      </ul>
      
      <p>Google 与 50 多个合作伙伴共同推动这一协议的发展，包括 Atlassian、Box、Cohere、LangChain、MongoDB、Salesforce、ServiceNow 等。</p>
      
      <p>阅读完整文章：<a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/</a></p>
    `,
    contentEn: `
      <p>Google announced the A2A (Agent-to-Agent) protocol, an open protocol that provides a standard way for agents to collaborate with each other, regardless of the underlying framework or vendor. This marks the beginning of a new era of agent interoperability.</p>
      
      <p>A2A follows five key design principles:</p>
      <ul>
        <li><b>Embrace agentic capabilities</b>: A2A focuses on enabling agents to collaborate in their natural, unstructured modalities, even when they don't share memory, tools and context.</li>
        <li><b>Build on existing standards</b>: The protocol is built on top of existing, popular standards including HTTP, SSE, JSON-RPC, making it easier to integrate with existing IT stacks.</li>
        <li><b>Secure by default</b>: A2A is designed to support enterprise-grade authentication and authorization, with parity to OpenAPI's authentication schemes at launch.</li>
        <li><b>Support for long-running tasks</b>: A2A is designed to be flexible and support scenarios from quick tasks to deep research that may take hours or even days.</li>
        <li><b>Modality agnostic</b>: The agentic world isn't limited to just text, which is why A2A supports various modalities, including audio and video streaming.</li>
      </ul>
      
      <p>Google is working with over 50 partners to advance this protocol, including Atlassian, Box, Cohere, LangChain, MongoDB, Salesforce, ServiceNow, and more.</p>
      
      <p>Read the full article at <a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/</a></p>
    `,
    tags: ['Google', 'A2A', 'Agent-Interoperability'],
    source: 'Google Developers Blog'
  },
  'anthropic-model-context-protocol': {
    titleZh: 'Anthropic 发布 Model Context Protocol (MCP)',
    titleEn: 'Anthropic Introduces Model Context Protocol (MCP)',
    date: '2024-11-25',
    contentZh: `
      <p>Anthropic 宣布开源 Model Context Protocol (MCP)，这是一个用于连接 AI 助手与数据所在系统的新标准，包括内容仓库、业务工具和开发环境。其目标是帮助前沿模型产生更好、更相关的响应。</p>
      
      <p>随着 AI 助手获得主流采用，行业在模型能力方面投入巨资，在推理和质量方面取得了快速进步。然而，即使是最复杂的模型也受到其与数据隔离的限制——被困在信息孤岛和遗留系统后面。每个新数据源都需要自己的自定义实现，使得真正互联的系统难以扩展。</p>
      
      <p>MCP 解决了这一挑战。它提供了一个通用的开放标准，用于将 AI 系统与数据源连接，用单一协议取代碎片化的集成。结果是 AI 系统访问所需数据的更简单、更可靠的方式。</p>
      
      <p>核心组件包括：</p>
      <ul>
        <li>MCP 规范和 SDK</li>
        <li>Claude Desktop 应用中的本地 MCP 服务器支持</li>
        <li>开源的 MCP 服务器仓库</li>
      </ul>
      
      <p>阅读完整文章：<a href="https://www.anthropic.com/news/model-context-protocol">https://www.anthropic.com/news/model-context-protocol</a></p>
    `,
    contentEn: `
      <p>Anthropic announced the open-source Model Context Protocol (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. Its aim is to help frontier models produce better, more relevant responses.</p>
      
      <p>As AI assistants gain mainstream adoption, the industry has invested heavily in model capabilities, achieving rapid advances in reasoning and quality. Yet even the most sophisticated models are constrained by their isolation from data—trapped behind information silos and legacy systems. Every new data source requires its own custom implementation, making truly connected systems difficult to scale.</p>
      
      <p>MCP addresses this challenge. It provides a universal, open standard for connecting AI systems with data sources, replacing fragmented integrations with a single protocol. The result is a simpler, more reliable way to give AI systems access to the data they need.</p>
      
      <p>Core components include:</p>
      <ul>
        <li>MCP specification and SDKs</li>
        <li>Local MCP server support in Claude Desktop apps</li>
        <li>An open-source repository of MCP servers</li>
      </ul>
      
      <p>Read the full article at <a href="https://www.anthropic.com/news/model-context-protocol">https://www.anthropic.com/news/model-context-protocol</a></p>
    `,
    tags: ['Anthropic', 'MCP', 'AI-Standards'],
    source: 'Anthropic'
  },
  'rynnbrain-alibaba-embodied-model': {
    titleZh: 'RynnBrain：阿里巴巴开源具身智能基础模型',
    titleEn: 'RynnBrain: Alibaba\'s Open Embodied Foundation Model',
    date: '2026-02-15',
    contentZh: `
      <p>阿里巴巴达摩院近日发布了 <strong>RynnBrain</strong>，一个基于物理现实的具身基础模型（Embodied Foundation Model）。该模型在物理世界理解、空间推理和机器人任务规划方面展现了强大的能力。</p>
      
      <h2>模型规格</h2>
      <p>RynnBrain 提供三种模型规格：</p>
      <ul>
        <li><strong>RynnBrain-2B</strong>：轻量级密集模型</li>
        <li><strong>RynnBrain-8B</strong>：标准密集模型</li>
        <li><strong>RynnBrain-30B-A3B</strong>：MoE（混合专家）模型，激活参数 3B</li>
      </ul>
      
      <h2>核心能力</h2>
      
      <h3>1. 全面的自我中心理解</h3>
      <p>在细粒度视频理解和自我中心认知方面表现出色，涵盖具身问答、计数和 OCR 等任务。</p>
      
      <h3>2. 多样化时空定位</h3>
      <p>具备强大的跨时间记忆定位能力，可精确识别物体、目标区域和运动轨迹。</p>
      
      <h3>3. 物理空间推理</h3>
      <p>采用文本和空间定位交替进行的交错推理策略，确保推理过程根植于物理环境。</p>
      
      <h3>4. 物理感知精确规划</h3>
      <p>将定位出的可供性和物体信息整合到规划中，使下游 VLA（视觉-语言-动作）模型能够执行具有细粒度指令的复杂任务。</p>
      
      <h2>专项模型</h2>
      <p>除基础模型外，达摩院还发布了三个后训练专项模型：</p>
      <ul>
        <li><strong>RynnBrain-Plan</strong>：机器人任务规划</li>
        <li><strong>RynnBrain-Nav</strong>：视觉语言导航</li>
        <li><strong>RynnBrain-CoP</strong>：链式点推理（Chain-of-Point）</li>
      </ul>
      
      <h2>技术报告与资源</h2>
      <p>达摩院同时发布了详细的技术报告，并在 Hugging Face 和 ModelScope 上开源了模型权重和代码。</p>
      
      <p><strong>相关链接：</strong></p>
      <ul>
        <li>项目主页：<a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/">https://alibaba-damo-academy.github.io/RynnBrain.github.io/</a></li>
        <li>GitHub 仓库：<a href="https://github.com/alibaba-damo-academy/RynnBrain">https://github.com/alibaba-damo-academy/RynnBrain</a></li>
        <li>在线 Demo：<a href="https://huggingface.co/spaces/Alibaba-DAMO-Academy/RynnBrain">Hugging Face Spaces</a></li>
        <li>技术报告：<a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/assets/RynnBrain_Report.pdf">PDF 下载</a></li>
      </ul>
    `,
    contentEn: `
      <p>Alibaba DAMO Academy has recently released <strong>RynnBrain</strong>, an embodied foundation model grounded in physical reality. The model demonstrates strong capabilities in physical world understanding, spatial reasoning, and robot task planning.</p>
      
      <h2>Model Specifications</h2>
      <p>RynnBrain is available in three variants:</p>
      <ul>
        <li><strong>RynnBrain-2B</strong>: Lightweight dense model</li>
        <li><strong>RynnBrain-8B</strong>: Standard dense model</li>
        <li><strong>RynnBrain-30B-A3B</strong>: MoE (Mixture-of-Experts) model with 3B active parameters</li>
      </ul>
      
      <h2>Core Capabilities</h2>
      
      <h3>1. Comprehensive Egocentric Understanding</h3>
      <p>Excels in fine-grained video understanding and egocentric cognition, covering tasks such as embodied QA, counting, and OCR.</p>
      
      <h3>2. Diverse Spatio-temporal Localization</h3>
      <p>Possesses powerful localization capabilities across episodic memory, enabling precise identification of objects, target areas, and motion trajectories.</p>
      
      <h3>3. Physical-space Reasoning</h3>
      <p>Employs an interleaved reasoning strategy that alternates between textual and spatial grounding, ensuring that reasoning processes are firmly rooted in the physical environment.</p>
      
      <h3>4. Physics-aware Precise Planning</h3>
      <p>Integrates located affordances and object information into planning, enabling downstream VLA (Vision-Language-Action) models to execute intricate tasks with fine-grained instructions.</p>
      
      <h2>Specialized Models</h2>
      <p>In addition to the base models, DAMO Academy has released three post-trained specialized models:</p>
      <ul>
        <li><strong>RynnBrain-Plan</strong>: Robot task planning</li>
        <li><strong>RynnBrain-Nav</strong>: Vision-language navigation</li>
        <li><strong>RynnBrain-CoP</strong>: Chain-of-Point reasoning</li>
      </ul>
      
      <h2>Technical Report and Resources</h2>
      <p>DAMO Academy has also published a detailed technical report and open-sourced model weights and code on Hugging Face and ModelScope.</p>
      
      <p><strong>Related Links:</strong></p>
      <ul>
        <li>Project Page: <a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/">https://alibaba-damo-academy.github.io/RynnBrain.github.io/</a></li>
        <li>GitHub Repository: <a href="https://github.com/alibaba-damo-academy/RynnBrain">https://github.com/alibaba-damo-academy/RynnBrain</a></li>
        <li>Live Demo: <a href="https://huggingface.co/spaces/Alibaba-DAMO-Academy/RynnBrain">Hugging Face Spaces</a></li>
        <li>Technical Report: <a href="https://alibaba-damo-academy.github.io/RynnBrain.github.io/assets/RynnBrain_Report.pdf">PDF Download</a></li>
      </ul>
    `,
    tags: ['AI', 'Embodied AI', 'Foundation Model', 'Robotics', 'Alibaba'],
    source: 'Alibaba DAMO Academy'
  },
  'figma-future-of-design-code-canvas': {
    titleZh: 'Figma：设计的未来是代码与画布',
    titleEn: 'Figma: The Future of Design is Code and Canvas',
    date: '2026-02-18',
    contentZh: `
      <p>Figma CEO Dylan Field 近日发表文章，阐述了 Figma 对设计未来的愿景：<strong>代码与画布的融合</strong>。在这个 AI 能力爆炸的时代，设计的核心工作是在无限可能性中找到最佳解决方案。</p>
      
      <h2>Claude Code to Figma：从代码到设计稿</h2>
      <p>Figma 推出了全新的 <strong>Claude Code to Figma</strong> 功能，让开发者可以直接将代码转换为可编辑的 Figma 设计稿：</p>
      <ul>
        <li>安装 Figma MCP（Model Context Protocol）</li>
        <li>在 Claude Code 中输入 "Send this to Figma"</li>
        <li>浏览器渲染状态自动转换为完全可编辑的 Figma 图层</li>
      </ul>
      
      <h2>画布思维 vs 代码思维</h2>
      <p>Figma 认为，在 AI 能够帮助构建任何你能描述的可能性的世界里，设计的核心工作是找到最佳解决方案。而设计画布比 IDE 或 ADE 中的提示更适合导航众多可能性：</p>
      <ul>
        <li><strong>发散思考</strong>：在画布上可以并排比较不同方案，看到全局</li>
        <li><strong>直接操作</strong>：可以直接操控编辑细节</li>
        <li><strong>双向流转</strong>：探索并打磨设计后，可以通过 Figma MCP 将设计变更拉回到代码库</li>
      </ul>
      
      <h2>工作流程的演进</h2>
      <p>过去，产品设计和开发工作流程通常是线性的：头脑风暴 → 设计 → 编码。现在，你可以从任何地方开始，到任何地方去。关键是停下来问自己是否在正确的方向上构建。</p>
      
      <p>Figma 希望帮助用户逃离"隧道视野"，跳出来探索全局。无论是产品构建始于终端、提示框、可视化 UI 还是手绘草图，Figma 都希望成为一切汇聚的地方。</p>
      
      <p>阅读完整文章：<a href="https://www.figma.com/blog/the-future-of-design-is-code-and-canvas/">https://www.figma.com/blog/the-future-of-design-is-code-and-canvas/</a></p>
    `,
    contentEn: `
      <p>Figma CEO Dylan Field recently published an article outlining Figma's vision for the future of design: <strong>the convergence of code and canvas</strong>. In this era of exploding AI capabilities, the core work of design is finding the best possible solutions in a nearly infinite possibility space.</p>
      
      <h2>Claude Code to Figma: From Code to Design</h2>
      <p>Figma has introduced the new <strong>Claude Code to Figma</strong> feature, allowing developers to directly convert code into editable Figma designs:</p>
      <ul>
        <li>Install the Figma MCP (Model Context Protocol)</li>
        <li>Type "Send this to Figma" in Claude Code</li>
        <li>The browser's rendered state automatically translates to fully editable Figma layers</li>
      </ul>
      
      <h2>Canvas Thinking vs. Code Thinking</h2>
      <p>Figma believes that in a world where AI can help build any possibility you can articulate, the core work of design is finding the best possible solutions. The design canvas is better at navigating lots of possibilities than prompting in an IDE or ADE:</p>
      <ul>
        <li><strong>Divergent thinking</strong>: On the canvas, you can compare approaches side by side and see the big picture</li>
        <li><strong>Direct manipulation</strong>: You can use direct manipulation to edit details</li>
        <li><strong>Bi-directional flow</strong>: After exploring options and polishing the design, you can simply use Figma MCP to pull design changes back into your codebase</li>
      </ul>
      
      <h2>Evolution of Workflows</h2>
      <p>Previously, product design and development workflows were often linear: brainstorm → design → code. Today, you can start anywhere and go everywhere. What's key is taking a moment to ask yourself if you're even building in the right direction.</p>
      
      <p>Figma hopes to help users escape "tunnel vision," zoom out, and explore the big picture. Whether product building begins in a terminal, a prompt box, a visual UI, or a hand-drawn sketch, Figma wants to be the place where it all comes together.</p>
      
      <p>Read the full article at <a href="https://www.figma.com/blog/the-future-of-design-is-code-and-canvas/">https://www.figma.com/blog/the-future-of-design-is-code-and-canvas/</a></p>
    `,
    tags: ['Figma', 'AI-Design', 'Claude', 'MCP', 'Product-Design'],
    source: 'Figma Blog'
  },
  'gemini-3-1-pro-model-card': {
    titleZh: 'Google 发布 Gemini 3.1 Pro：新一代多模态推理模型',
    titleEn: 'Google Releases Gemini 3.1 Pro: Next-Gen Multimodal Reasoning Model',
    date: '2026-02-20',
    contentZh: `
      <p>Google DeepMind 发布 <strong>Gemini 3.1 Pro</strong> 模型卡，这是 Gemini 3 系列的最新迭代，也是 Google 目前最先进的复杂任务处理模型。作为原生多模态推理模型，Gemini 3.1 Pro 能够处理来自文本、音频、图像、视频和完整代码库的大规模多模态信息。</p>
      
      <h2>核心规格</h2>
      <ul>
        <li><strong>上下文窗口</strong>：100 万 tokens</li>
        <li><strong>输出长度</strong>：64K tokens</li>
        <li><strong>输入类型</strong>：文本、图像、音频、视频</li>
        <li><strong>基础模型</strong>：基于 Gemini 3 Pro 架构</li>
      </ul>
      
      <h2>性能表现</h2>
      <p>Gemini 3.1 Pro 在多个关键基准测试中显著超越 Gemini 3 Pro 和竞争对手：</p>
      
      <h3>学术推理</h3>
      <ul>
        <li><strong>Humanity's Last Exam</strong>：44.4%（无工具）/ 51.4%（搜索+代码）- 业界领先</li>
        <li><strong>ARC-AGI-2</strong>：77.1% - 抽象推理难题</li>
        <li><strong>GPQA Diamond</strong>：94.3% - 科学知识问答</li>
      </ul>
      
      <h3>编程能力</h3>
      <ul>
        <li><strong>SWE-Bench Verified</strong>：80.6% - 智能体编程</li>
        <li><strong>Terminal-Bench 2.0</strong>：68.5% - 终端代码代理</li>
        <li><strong>LiveCodeBench Pro</strong>：Elo 2887 - 竞争性编程</li>
      </ul>
      
      <h3>多模态与长上下文</h3>
      <ul>
        <li><strong>MMMU-Pro</strong>：80.5% - 多模态理解</li>
        <li><strong>MRCR v2 (128k)</strong>：84.9% - 长上下文检索</li>
        <li><strong>MMMLU</strong>：92.6% - 多语言问答</li>
      </ul>
      
      <h3>智能体能力</h3>
      <ul>
        <li><strong>APEX-Agents</strong>：33.5% - 长周期专业任务</li>
        <li><strong>BrowseComp</strong>：85.9% - 智能体搜索</li>
        <li><strong>MCP Atlas</strong>：69.2% - 多步工作流</li>
      </ul>
      
      <h2>应用场景</h2>
      <p>Gemini 3.1 Pro 特别适合以下应用：</p>
      <ul>
        <li>智能体性能任务</li>
        <li>高级编程和代码生成</li>
        <li>长上下文和多模态理解</li>
        <li>算法开发</li>
        <li>复杂问题求解和战略规划</li>
      </ul>
      
      <p>阅读完整模型卡：<a href="https://deepmind.google/models/model-cards/gemini-3-1-pro/">https://deepmind.google/models/model-cards/gemini-3-1-pro/</a></p>
    `,
    contentEn: `
      <p>Google DeepMind has released the <strong>Gemini 3.1 Pro</strong> model card, the latest iteration in the Gemini 3 series and Google's most advanced model for complex tasks. As a natively multimodal reasoning model, Gemini 3.1 Pro can process massively multimodal information from text, audio, images, video, and entire code repositories.</p>
      
      <h2>Core Specifications</h2>
      <ul>
        <li><strong>Context Window</strong>: 1 million tokens</li>
        <li><strong>Output Length</strong>: 64K tokens</li>
        <li><strong>Input Types</strong>: Text, images, audio, video</li>
        <li><strong>Base Architecture</strong>: Based on Gemini 3 Pro</li>
      </ul>
      
      <h2>Performance</h2>
      <p>Gemini 3.1 Pro significantly outperforms Gemini 3 Pro and competitors across key benchmarks:</p>
      
      <h3>Academic Reasoning</h3>
      <ul>
        <li><strong>Humanity's Last Exam</strong>: 44.4% (no tools) / 51.4% (search+code) - Industry leading</li>
        <li><strong>ARC-AGI-2</strong>: 77.1% - Abstract reasoning puzzles</li>
        <li><strong>GPQA Diamond</strong>: 94.3% - Scientific knowledge Q&A</li>
      </ul>
      
      <h3>Coding Capabilities</h3>
      <ul>
        <li><strong>SWE-Bench Verified</strong>: 80.6% - Agentic coding</li>
        <li><strong>Terminal-Bench 2.0</strong>: 68.5% - Terminal code agent</li>
        <li><strong>LiveCodeBench Pro</strong>: Elo 2887 - Competitive programming</li>
      </ul>
      
      <h3>Multimodal & Long Context</h3>
      <ul>
        <li><strong>MMMU-Pro</strong>: 80.5% - Multimodal understanding</li>
        <li><strong>MRCR v2 (128k)</strong>: 84.9% - Long context retrieval</li>
        <li><strong>MMMLU</strong>: 92.6% - Multilingual Q&A</li>
      </ul>
      
      <h3>Agentic Capabilities</h3>
      <ul>
        <li><strong>APEX-Agents</strong>: 33.5% - Long horizon professional tasks</li>
        <li><strong>BrowseComp</strong>: 85.9% - Agentic search</li>
        <li><strong>MCP Atlas</strong>: 69.2% - Multi-step workflows</li>
      </ul>
      
      <h2>Use Cases</h2>
      <p>Gemini 3.1 Pro is particularly well-suited for:</p>
      <ul>
        <li>Agentic performance tasks</li>
        <li>Advanced coding and code generation</li>
        <li>Long context and multimodal understanding</li>
        <li>Algorithm development</li>
        <li>Complex problem solving and strategic planning</li>
      </ul>
      
      <p>Read the full model card at <a href="https://deepmind.google/models/model-cards/gemini-3-1-pro/">https://deepmind.google/models/model-cards/gemini-3-1-pro/</a></p>
    `,
    tags: ['Google', 'Gemini', 'AI-Model', 'Multimodal', 'Reasoning', 'Agentic-AI'],
    source: 'Google DeepMind'
  }
};

const post = posts[slug];
---

<Layout>
  <article class="article-content">
    <div class="lang-content lang-zh active">
      <h1>{post.titleZh}</h1>
    </div>
    <div class="lang-content lang-en">
      <h1>{post.titleEn}</h1>
    </div>
    <time>{post.date}</time>
    
    <div class="lang-content lang-zh active" set:html={post.contentZh} />
    <div class="lang-content lang-en" set:html={post.contentEn} />
    
    <div style="margin-top: 2.5rem; padding-top: 1.5rem; border-top: 1px solid var(--color-border);">
      <div class="tags">
        {post.tags.map(tag => (
          <span>{tag}</span>
        ))}
      </div>
    </div>
  </article>
  
  <a href="/" class="back-link" data-zh="← 全部文章" data-en="← All articles">← All articles</a>
</Layout>
